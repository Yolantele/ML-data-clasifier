{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpaCy-ML-Classifier-for-Waste-Data-Augmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOtYrsRgXm8uh2LgVtNVNPW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yolantele/ML-data-clasifier/blob/master/SpaCy_ML_Classifier_for_Waste_Data_Augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US3U82Q2Oq28",
        "colab_type": "text"
      },
      "source": [
        "####**SpaCy Data Classification POC**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0MIMTSkPAN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GchVo49oPHvN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "9fb32076-90b9-483d-b98d-0a771275f487"
      },
      "source": [
        "# mount data from drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/My Drive/data/'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTRIFjFaPbf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U spacy==2.2.2\n",
        "!pip install pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxLC2X13Qw7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import pandas as pd\n",
        "from spacy.lang.en import English\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# References:\n",
        "# https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/\n",
        "\n",
        "spacy.prefer_gpu()\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L324D-Gql23q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "ed8799a3-bf3f-4eac-cac2-afd4c5636e3f"
      },
      "source": [
        "materials = pd.read_csv(path + '/enMaterialData.csv')\n",
        "# or use test data frame where material field is empty\n",
        "materials_test = pd.read_csv(path + '/enWithoutMaterialData.csv')\n",
        "\n",
        "df = materials\n",
        "# df.head()\n",
        "# df.info()\n",
        "# df.description + df.euralDescription\n",
        "df.description + df.euralDescription\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       Soyadroes technicallymaterial unsuitable for c...\n",
              "1                             Brancheswaste from forestry\n",
              "2       Cocoa shellsmaterial unsuitable for consumptio...\n",
              "3       Debris with Sandmixtures of concrete, stones, ...\n",
              "4       soyamaterial unsuitable for consumption or pro...\n",
              "                              ...                        \n",
              "2115                                  LDPE - 98:2plastics\n",
              "2116    hollow variegated glass, route collection fire...\n",
              "2117    Soil mixed with contaminated stonessoil and st...\n",
              "2118    Stainless steel, Motors, Refiner, Tin / Lead, ...\n",
              "2119    soil / gravel / stones / cloths oil contaminat...\n",
              "Length: 2120, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPOLLyVRzcQ9",
        "colab_type": "text"
      },
      "source": [
        "###Tokening the Data With spaCy\n",
        "\n",
        "Now that we know what we’re working with, let’s create a custom tokenizer function using spaCy. We’ll use this function to automatically strip information we don’t need, like stopwords and punctuation, from each review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaimUuePV93l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1475913f-896c-4f6a-c012-3176b4ab659a"
      },
      "source": [
        "\n",
        "string = \"mixtures of concrete, stones, tiles or ceramic products other than those mentioned in 17 01 06\"\n",
        "string2 = 'B and O land class LIVING - B'\n",
        "\n",
        "punctuations = '!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~'\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "def spacy_tokenizes(docs, logging=False):\n",
        "    texts = []\n",
        "    doc = nlp(docs, disable=['parser', 'ner'])\n",
        "    tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
        "    tokens = [tok for tok in tokens if tok not in punctuations  and tok not in stop_words ]\n",
        "    return tokens\n",
        "\n",
        "tokens = spacy_tokenizes(string2)\n",
        "print(tokens)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['b', 'o', 'land', 'class', 'living', 'b']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDWKUU8RvJz9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4a25b106-4c1e-4d24-8544-a6ab474f9cd8"
      },
      "source": [
        "# Different tokenizer (leaves words in plurals)\n",
        "parser = English()\n",
        "\n",
        "def spacy_tokenize(sentence):\n",
        "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
        "    sentence = sentence.strip().lower()\n",
        "    mytokens = parser(sentence)\n",
        "\n",
        "    # Lemmatizing each token and converting each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Removing stop words\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    # return preprocessed list of tokens\n",
        "    return mytokens\n",
        "\n",
        "tokens = spacy_tokenize(string2)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['b', 'o', 'land', 'class', 'living', 'b']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj7q5mGz0V6Q",
        "colab_type": "text"
      },
      "source": [
        "###Vectorization Feature Engineering (TF-IDF) , Bag of Words and N-grams\n",
        "\n",
        "Classifying text we end up with text snippets with their respective labels. But in machine learning model we need to convert into numeric representation\n",
        "\n",
        "TF-IDF -Term Frequency-Inverse Document Frequency - simply a way of normalizing our Bag of Words(BoW) by looking at each word’s frequency in comparison to the document frequency.\n",
        "\n",
        "N-grams - combinations of adjacent words in a given text. For example \"who will win\"\n",
        "- when n = 1, becomes \"who\", \"will\", \"win\"\n",
        "- when n = 2 , becomes \"who will\", \"will win\" etc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8vNowrk0WUW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "d4af7715-9dab-4a59-8b45-229f7972cdb3"
      },
      "source": [
        "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n",
        "\n",
        "print(bow_vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9582735c9f8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbow_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-bT6HzllahL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}